{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-0--3I-O1mA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Confluence Data"
      ],
      "metadata": {
        "id": "azb-Id5oP7_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install atlassian-python-api\n",
        "\n",
        "from atlassian import Confluence\n",
        "import os\n",
        "\n",
        "# Set up Confluence API connection\n",
        "confluence = Confluence(\n",
        "url='https://advendio.atlassian.net',\n",
        ")\n",
        "confluence\n",
        "space_key = \"SO\"\n",
        "pages = confluence.get_all_pages_from_space(space_key)\n",
        "pages\n",
        "# Create a directory to store the downloaded pages\n",
        "if not os.path.exists('advendio_pages'):\n",
        "    os.makedirs('advendio_pages')\n",
        "# Download each page\n",
        "for page in pages:\n",
        "    page_id = page['id']\n",
        "    page_title = page['title']\n",
        "    page_filename = page_title.replace(' ', '_') + '.html'\n",
        "    page_content = confluence.get_page_by_id(page_id, expand='body.storage')['body']['storage']['value']\n",
        "    try:\n",
        "        with open('advendio_pages/' + page_filename, 'w') as f:\n",
        "            f.write(page_content)\n",
        "    except:\n",
        "        pass\n",
        "    print('Downloaded:', page_filename)\n"
      ],
      "metadata": {
        "id": "jsnW6c4MP_WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import faiss\n",
        "\n",
        "documents = []\n",
        "for filename in os.listdir('advendio_pages'):\n",
        "  f = os.path.join('advendio_pages', filename)\n",
        "  with open(f, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "    soup = BeautifulSoup(html_content, \"lxml\")\n",
        "\n",
        "    text_content = soup.get_text(separator=\" \", strip=True)\n",
        "    documents.append(text_content)\n",
        "  "
      ],
      "metadata": {
        "id": "udTC63zjP_uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "9YqIRYsuSWca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use DPR Question and Context Encoders \n"
      ],
      "metadata": {
        "id": "M21sIkDIedGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "import torch\n",
        "\n",
        "# User query\n",
        "query = \"brown fox\"\n",
        "\n",
        "\n",
        "\n",
        "# Load DPR question and context encoders\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# Encode the documents\n",
        "encoded_documents = context_tokenizer(documents, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "document_embeddings = context_encoder(**encoded_documents).pooler_output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6prmgAS1O86O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create FAISS Index"
      ],
      "metadata": {
        "id": "tuLUCdN_ejKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "document_embeddings = document_embeddings.detach().numpy()\n",
        "document_embeddings=np.ascontiguousarray(document_embeddings)\n",
        "\n",
        "\n",
        "# Create Faiss Index\n",
        "vector_dimension = document_embeddings.shape[1]\n",
        "print(vector_dimension)\n",
        "index = faiss.IndexFlatL2(vector_dimension)\n",
        "faiss.normalize_L2(document_embeddings)\n",
        "index.add(document_embeddings)\n",
        "print(index.ntotal)\n"
      ],
      "metadata": {
        "id": "0f4-c1REazqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Retrieval"
      ],
      "metadata": {
        "id": "UDmUtgyHepTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "question = input(\"Enter your prompt: \")\n",
        "\n",
        "# # Encode the query\n",
        "encoded_query = question_tokenizer(query, return_tensors=\"pt\")\n",
        "query_embedding = question_encoder(**encoded_query).pooler_output.detach().numpy()\n",
        "query_embedding=np.ascontiguousarray(query_embedding)\n",
        "\n",
        "D, I = index.search(query_embedding, 4)\n",
        "print(I)\n",
        "\n",
        "# # Compute cosine similarity between the query embedding and document embeddings\n",
        "# cosine_similarities = torch.mm(query_embedding, document_embeddings.T).squeeze(0)\n",
        "\n",
        "# # Rank the documents based on cosine similarity scores\n",
        "# ranked_documents = cosine_similarities.argsort(descending=True)\n",
        "\n",
        "# # Display the ranked documents\n",
        "# for index in ranked_documents:\n",
        "#     print(f\"Document {index + 1} (Score: {cosine_similarities[index].item():.4f}): {documents[index]}\")\n"
      ],
      "metadata": {
        "id": "TlzdHBLWTInq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[11]"
      ],
      "metadata": {
        "id": "XgmzakpLUgab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Response"
      ],
      "metadata": {
        "id": "fh3njKlmeufq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"microsoft/GODEL-v1_1-large-seq2seq\"\n",
        "        )\n",
        "qa_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            \"microsoft/GODEL-v1_1-large-seq2seq\"\n",
        "        )\n",
        "knowledge = documents[I[0][0]]\n",
        "knowledge = \"[KNOWLEDGE] \" + knowledge\n",
        "dialog = [question]\n",
        "dialog = \" EOS \".join(dialog)\n",
        "instruction = (\n",
        "            f\"Instruction: given a dialog context, you need to response empathically.\"\n",
        "        )\n",
        "query = f\"{instruction} [CONTEXT] {dialog} {knowledge}\"\n",
        "print(query)\n",
        "input_ids = qa_tokenizer(f\"{query}\", return_tensors=\"pt\").input_ids\n",
        "output = qa_model.generate(\n",
        "            input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True\n",
        "        )\n",
        "output = qa_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print('Response: {}'.format(output))"
      ],
      "metadata": {
        "id": "sYHKMJ8ad1sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vn1iAKt9eT0d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}