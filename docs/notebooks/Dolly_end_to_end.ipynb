{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvl11SfR53f8"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate>=0.12.0 transformers[torch]==4.25.1 \n",
        "!pip install -U \"ray[serve]\"  # installs Ray + dependencies for Ray Serve\n",
        "!pip install -U slack-bolt\n",
        "!pip install faiss-cpu\n",
        "!pip install atlassian-python-api\n",
        "!pip install numpy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "from fastapi import FastAPI, Request\n",
        "from ray import serve\n",
        "from slack_bolt.async_app import AsyncApp\n",
        "from slack_bolt.adapter.fastapi.async_handler import AsyncSlackRequestHandler\n",
        "from slack_bolt.adapter.starlette.handler import SlackRequestHandler\n",
        "import requests\n",
        "from slack_sdk.signature import SignatureVerifier\n",
        "import ray\n",
        "import asyncio\n",
        "\n",
        "import logging\n",
        "# Configure the logger.\n",
        "\n",
        "from atlassian import Confluence\n",
        "import os\n",
        "\n",
        "# Set up Confluence API connection\n",
        "confluence = Confluence(\n",
        "url='https://advendio.atlassian.net',\n",
        ")\n",
        "\n",
        "space_key = \"SO\"\n",
        "pages = confluence.get_all_pages_from_space(space_key)\n",
        "\n",
        "# Create a directory to store the downloaded pages\n",
        "if not os.path.exists('advendio_pages'):\n",
        "    os.makedirs('advendio_pages')\n",
        "# Download each page\n",
        "for page in pages:\n",
        "    page_id = page['id']\n",
        "    page_title = page['title']\n",
        "    page_filename = page_title.replace(' ', '_') + '.html'\n",
        "    page_content = confluence.get_page_by_id(page_id, expand='body.storage')['body']['storage']['value']\n",
        "    try:\n",
        "        with open('advendio_pages/' + page_filename, 'w') as f:\n",
        "            f.write(page_content)\n",
        "    except:\n",
        "        pass\n",
        "    print('Downloaded:', page_filename)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from typing import List\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import faiss\n",
        "\n",
        "@serve.deployment(ray_actor_options={\"num_gpus\": 0.5})\n",
        "class DocumentVectorDB:\n",
        "    def __init__(self, \n",
        "                 question_encoder_model: str = \"facebook/dpr-question_encoder-single-nq-base\",\n",
        "                 context_encoder: str = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "                 ):\n",
        "        self.token_limit = 512\n",
        "\n",
        "        self.documents = self.format_documents()\n",
        "        self.question_encoder = DPRQuestionEncoder.from_pretrained(\n",
        "            question_encoder_model)\n",
        "        self.question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n",
        "            question_encoder_model)\n",
        "        self.context_encoder = DPRContextEncoder.from_pretrained(\n",
        "            context_encoder)\n",
        "        self.context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\n",
        "            context_encoder)\n",
        "        count = self.index_documents(self.documents)\n",
        "        print(\"document count:{}\".format(count))\n",
        "      \n",
        "    def index_documents(self, documents: List[str]) -> int:\n",
        "        # Encode the documents\n",
        "        encoded_documents = self.context_tokenizer(\n",
        "            self.documents, \n",
        "            return_tensors=\"pt\", \n",
        "            padding=True, \n",
        "            truncation=True, \n",
        "            max_length=self.token_limit)\n",
        "        document_embeddings = self.context_encoder(**encoded_documents).pooler_output\n",
        "\n",
        "        document_embeddings = document_embeddings.detach().numpy()\n",
        "        document_embeddings=np.ascontiguousarray(document_embeddings)\n",
        "        # Create Faiss Index\n",
        "        vector_dimension = document_embeddings.shape[1]\n",
        "        print(\"vector dimension:{}\".format(vector_dimension))\n",
        "        self.index = faiss.IndexFlatL2(vector_dimension)\n",
        "        faiss.normalize_L2(document_embeddings)\n",
        "        self.index.add(document_embeddings)\n",
        "        return self.index.ntotal\n",
        "\n",
        "    def insert_documents(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        store some whre\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def encode_questions(self, query: str) -> List[torch.Tensor]:\n",
        "        encoded_query = self.question_tokenizer(query, return_tensors=\"pt\")\n",
        "        query_embedding = self.question_encoder(\n",
        "            **encoded_query).pooler_output.detach().numpy()\n",
        "        query_embedding = np.ascontiguousarray(query_embedding)\n",
        "        return query_embedding\n",
        "\n",
        "    def query_documents(self, query: str) -> str:\n",
        "        # Encode the query\n",
        "        query_embedding = self.encode_questions(query)\n",
        "        _, idx = self.index.search(query_embedding, 4)\n",
        "        doc = self.documents[idx[0][0]]\n",
        "        print(\"query result: index={}, doc={}\".format(idx[0][0], doc))\n",
        "        return doc\n",
        "\n",
        "    def format_documents(self):\n",
        "        documents = []\n",
        "        for filename in os.listdir('advendio_pages'):\n",
        "            f = os.path.join('advendio_pages', filename)\n",
        "            with open(f, 'r', encoding='utf-8') as file:\n",
        "                html_content = file.read()\n",
        "                soup = BeautifulSoup(html_content, \"lxml\")\n",
        "\n",
        "                text_content = soup.get_text(separator=\" \", strip=True)\n",
        "                documents.append(text_content)\n",
        "        return documents\n",
        "\n",
        "@serve.deployment(route_prefix=\"/\", ray_actor_options={\"num_gpus\": 0.5})\n",
        "class RAGConversationBot:\n",
        "    def __init__(self, \n",
        "                 db: DocumentVectorDB, \n",
        "                 model: str = \"databricks/dolly-v2-3b\"):\n",
        "        self.model = model\n",
        "        self.db = db\n",
        "\n",
        "    async def prompt(self, input: str) ->str:\n",
        "        context_ref = await self.db.query_documents.remote(input)\n",
        "        context = await context_ref\n",
        "        assert isinstance(context, str)\n",
        "        return \"{} \\n context: {}\\n output:\".format(input, context)\n",
        "         \n",
        "    # Change this method to be an async function\n",
        "    async def generate_text(self, input_text: str) -> str:\n",
        "        generator = pipeline(model=self.model, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n",
        "        prompt_text = await self.prompt(input_text)\n",
        "        assert isinstance(prompt_text, str)\n",
        "        return generator(prompt_text)[0]\n",
        "\n",
        "    async def __call__(self, http_request: Request) -> str:\n",
        "        input_text: str = await http_request.json()\n",
        "        return await self.generate_text(input_text)\n",
        "\n",
        "# 2: Deploy the model.\n",
        "serve.run(RAGConversationBot.bind(DocumentVectorDB.bind()))\n",
        "\n",
        "english_text = \"How is an Ads event stored in ADvendio?\"\n",
        "# 3: Query the deployment and print the result.\n",
        "response = requests.post(\"http://127.0.0.1:8000/\", json=english_text)\n",
        "french_text = response.text\n",
        "print(french_text)\n"
      ],
      "metadata": {
        "id": "i_qFTMra57WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if ray.is_initialized():\n",
        "    ray.shutdown()\n",
        "ray.init()\n",
        "serve.start()\n",
        "# Start deployment instances.\n",
        "model_one.deploy()\n",
        "model_two.deploy()\n",
        "ComposedModel.deploy()\n",
        "\n",
        "# Now send requests.\n",
        "for _ in range(8):\n",
        "    resp = requests.get(\"http://127.0.0.1:8000/composed\", data=\"Hey!\")\n",
        "    print(resp.json())\n",
        "\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "id": "yd8DbkZy6JgF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}